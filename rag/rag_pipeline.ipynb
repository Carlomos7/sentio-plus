{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# RAG Application with AWS Bedrock & ChromaDB (Cloud)\n",
                "## Phase 1: Setup & Configuration\n",
                "This notebook covers the setup of dependencies, configuration of credentials, and initialization of AWS Bedrock and ChromaDB Cloud clients."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: boto3 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.42.23)\n",
                        "Requirement already satisfied: chromadb in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.4.0)\n",
                        "Requirement already satisfied: langchain in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.2.1)\n",
                        "Requirement already satisfied: langchain-community in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.4.1)\n",
                        "Requirement already satisfied: langchain-aws in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.2.0)\n",
                        "Requirement already satisfied: langchain-text-splitters in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.1.0)\n",
                        "Requirement already satisfied: botocore<1.43.0,>=1.42.23 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from boto3) (1.42.23)\n",
                        "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from boto3) (1.0.1)\n",
                        "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from boto3) (0.16.0)\n",
                        "Requirement already satisfied: build>=1.0.3 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (1.3.0)\n",
                        "Requirement already satisfied: pydantic>=1.9 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (2.11.5)\n",
                        "Requirement already satisfied: pybase64>=1.4.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (1.4.3)\n",
                        "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
                        "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (2.2.6)\n",
                        "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (5.4.0)\n",
                        "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (4.13.2)\n",
                        "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (1.23.2)\n",
                        "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (1.39.1)\n",
                        "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (1.39.1)\n",
                        "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (1.39.1)\n",
                        "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (0.22.1)\n",
                        "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (0.48.9)\n",
                        "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (4.67.1)\n",
                        "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (7.7.0)\n",
                        "Requirement already satisfied: importlib-resources in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (6.5.2)\n",
                        "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (1.74.0)\n",
                        "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (5.0.0)\n",
                        "Requirement already satisfied: typer>=0.9.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (0.21.0)\n",
                        "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (34.1.0)\n",
                        "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (8.5.0)\n",
                        "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (6.0.3)\n",
                        "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (5.2.0)\n",
                        "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (3.10.18)\n",
                        "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (0.28.1)\n",
                        "Requirement already satisfied: rich>=10.11.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (14.2.0)\n",
                        "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from chromadb) (4.25.1)\n",
                        "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (1.2.6)\n",
                        "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain) (1.0.5)\n",
                        "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (1.0.1)\n",
                        "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (2.0.45)\n",
                        "Requirement already satisfied: requests<3.0.0,>=2.32.5 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (2.32.5)\n",
                        "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (3.12.13)\n",
                        "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (0.6.7)\n",
                        "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (2.12.0)\n",
                        "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (0.6.1)\n",
                        "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-community) (0.4.3)\n",
                        "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
                        "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
                        "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
                        "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
                        "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
                        "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
                        "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
                        "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from botocore<1.43.0,>=1.42.23->boto3) (2.9.0.post0)\n",
                        "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from botocore<1.43.0,>=1.42.23->boto3) (2.3.0)\n",
                        "Requirement already satisfied: packaging>=19.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
                        "Requirement already satisfied: pyproject_hooks in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
                        "Requirement already satisfied: colorama in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
                        "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
                        "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
                        "Requirement already satisfied: anyio in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
                        "Requirement already satisfied: certifi in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
                        "Requirement already satisfied: httpcore==1.* in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
                        "Requirement already satisfied: idna in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
                        "Requirement already satisfied: h11>=0.16 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
                        "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
                        "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
                        "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
                        "Requirement already satisfied: six>=1.9.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
                        "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kubernetes>=28.1.0->chromadb) (2.40.3)\n",
                        "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
                        "Requirement already satisfied: requests-oauthlib in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
                        "Requirement already satisfied: durationpy>=0.7 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
                        "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
                        "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
                        "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
                        "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
                        "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
                        "Requirement already satisfied: xxhash>=3.5.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
                        "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
                        "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\n",
                        "Requirement already satisfied: coloredlogs in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
                        "Requirement already satisfied: flatbuffers in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
                        "Requirement already satisfied: protobuf in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.28.3)\n",
                        "Requirement already satisfied: sympy in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
                        "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
                        "Requirement already satisfied: googleapis-common-protos~=1.57 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
                        "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
                        "Requirement already satisfied: opentelemetry-proto==1.39.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
                        "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b1)\n",
                        "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
                        "Requirement already satisfied: distro>=1.5.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
                        "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
                        "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
                        "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
                        "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.2)\n",
                        "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
                        "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
                        "Requirement already satisfied: greenlet>=1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
                        "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
                        "Requirement already satisfied: click>=8.0.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
                        "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
                        "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
                        "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
                        "Requirement already satisfied: websockets>=10.4 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
                        "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
                        "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
                        "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
                        "Requirement already satisfied: filelock in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.2)\n",
                        "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.12.0)\n",
                        "Requirement already satisfied: zipp>=3.20 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
                        "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
                        "Requirement already satisfied: ormsgpack>=1.12.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
                        "Requirement already satisfied: mdurl~=0.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
                        "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
                        "Requirement already satisfied: sniffio>=1.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
                        "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
                        "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
                        "Requirement already satisfied: pyreadline3 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
                        "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\hayde\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
                        "Note: you may need to restart the kernel to use updated packages.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[notice] A new release of pip is available: 24.0 -> 25.3\n",
                        "[notice] To update, run: C:\\Users\\hayde\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
                    ]
                }
            ],
            "source": [
                "# Step 1: Install Dependencies\n",
                "# Using %pip ensures packages are installed in the current Jupyter kernel\n",
                "%pip install boto3 chromadb langchain langchain-community langchain-aws langchain-text-splitters dotenv"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Configuration Loaded.\n"
                    ]
                }
            ],
            "source": [
                "# Step 2: Configuration & Variables\n",
                "import os\n",
                "import chromadb\n",
                "from chromadb.config import Settings\n",
                "from langchain_experimental.text_splitter import SemanticChunker\n",
                "\n",
                "# --- AWS Configuration ---\n",
                "# PLEASE REPLACE WITH YOUR ACTUAL CREDENTIALS\n",
                "AWS_ACCESS_KEY_ID = os.environ.get[\"AWS_ACCESS_KEY_ID\", \"AKIAXXXEXAMPLE\"]\n",
                "AWS_SECRET_ACCESS_KEY = os.environ.get[\"AWS_SECRET_ACCESS_KEY\", \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"]\n",
                "AWS_REGION = \"us-west-2\"\n",
                "\n",
                "# --- Bedrock Model Configuration ---\n",
                "# Using a stable Claude 3 Sonnet ID which is widely available in us-west-2\n",
                "BEDROCK_MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
                "\n",
                "# --- ChromaDB Cloud Configuration ---\n",
                "# Sign up at https://trychroma.com to get your API Token\n",
                "CHROMA_DATABASE = \"sentio\" # Usually 'default_database'\n",
                "CHROMA_COLLECTION_NAME = \"rag_collection\"\n",
                "\n",
                "# Apply Environment Variables for Boto3\n",
                "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
                "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
                "os.environ[\"AWS_DEFAULT_REGION\"] = AWS_REGION\n",
                "\n",
                "print(\"Configuration Loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1. Initializing Boto3 Session...\n",
                        "   ✅ Bedrock Client Initialized successfully.\n",
                        "\n",
                        "2. Initializing ChromaDB Cloud Client...\n",
                        "   ✅ Connected to Chroma Cloud. Collection 'rag_collection' ready.\n",
                        "   ℹ️ Current Collection Count: 0\n"
                    ]
                }
            ],
            "source": [
                "# Step 3: Initialize Clients\n",
                "import boto3\n",
                "import chromadb\n",
                "\n",
                "print(\"1. Initializing Boto3 Session...\")\n",
                "try:\n",
                "    session = boto3.Session(\n",
                "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
                "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
                "        region_name=AWS_REGION\n",
                "    )\n",
                "    bedrock_client = session.client(\"bedrock-runtime\")\n",
                "    print(\"   ✅ Bedrock Client Initialized successfully.\")\n",
                "except Exception as e:\n",
                "    print(f\"   ❌ Error initializing Bedrock: {e}\")\n",
                "\n",
                "print(\"\\n2. Initializing ChromaDB Cloud Client...\")\n",
                "try:\n",
                "    # Initialize CloudClient specifically for Chroma Cloud\n",
                "    chroma_client = chromadb.PersistentClient(path=\"./my_chroma_data\")\n",
                "    \n",
                "    # Get or create the collection\n",
                "    collection = chroma_client.get_or_create_collection(name=CHROMA_COLLECTION_NAME)\n",
                "    print(f\"   ✅ Connected to Chroma Cloud. Collection '{CHROMA_COLLECTION_NAME}' ready.\")\n",
                "    print(f\"   ℹ️ Current Collection Count: {collection.count()}\")\n",
                "except Exception as e:\n",
                "    print(f\"   ❌ Error initializing ChromaDB Cloud: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 2: Data Ingestion & Chunking\n",
                "We will read text files from the `files/` directory, chunk them using LangChain's `RecursiveCharacterTextSplitter`, save the chunks to `files/chunked/`, and verify the output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "ℹ️ Directory exists: files\\chunked\n",
                        "Found 95 files in files: ['academic_and_professional_preparation_requirements_for_faculty.txt', 'academic_credit_policy.txt', 'academic_integrity_monitoring.txt', 'academic_progress_policy_as_bus_jepson.txt', 'acceptable_use_policy.txt'] ...\n"
                    ]
                }
            ],
            "source": [
                "# Step 4: Setup Directories\n",
                "import os\n",
                "\n",
                "SOURCE_DIR = \"files\"\n",
                "CHUNKED_DIR = os.path.join(SOURCE_DIR, \"chunked\")\n",
                "\n",
                "# Create chunked directory if it doesn't exist\n",
                "if not os.path.exists(CHUNKED_DIR):\n",
                "    os.makedirs(CHUNKED_DIR)\n",
                "    print(f\"✅ Created directory: {CHUNKED_DIR}\")\n",
                "else:\n",
                "    print(f\"ℹ️ Directory exists: {CHUNKED_DIR}\")\n",
                "\n",
                "# List source files (excluding directory or hidden files)\n",
                "source_files = [f for f in os.listdir(SOURCE_DIR) if os.path.isfile(os.path.join(SOURCE_DIR, f)) and not f.startswith('.')]\n",
                "print(f\"Found {len(source_files)} files in {SOURCE_DIR}: {source_files[:5]} ...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package punkt to\n",
                        "[nltk_data]     C:\\Users\\hayde\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt_tab to\n",
                        "[nltk_data]     C:\\Users\\hayde\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Package punkt_tab is already up-to-date!\n",
                        "C:\\Users\\hayde\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "C:\\Users\\hayde\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
                        "  if not hasattr(np, \"object\"):\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "WARNING:tensorflow:From C:\\Users\\hayde\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "import nltk\n",
                "nltk.download(\"punkt\")\n",
                "nltk.download(\"punkt_tab\")\n",
                "\n",
                "#from torch.optim.lr_scheduler import LRScheduler\n",
                "\n",
                "from nltk.tokenize import sent_tokenize\n",
                "from sentence_transformers import SentenceTransformer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SemanticChunker:\n",
                "    def __init__(\n",
                "        self,\n",
                "        model_name=\"all-MiniLM-L6-v2\",\n",
                "        similarity_threshold=0.75,\n",
                "        max_chunk_sentences=10\n",
                "    ):\n",
                "        self.model = SentenceTransformer(model_name)\n",
                "        self.similarity_threshold = similarity_threshold\n",
                "        self.max_chunk_sentences = max_chunk_sentences\n",
                "\n",
                "    def chunk(self, text):\n",
                "        sentences = sent_tokenize(text)\n",
                "\n",
                "        if len(sentences) == 0:\n",
                "            return []\n",
                "\n",
                "        embeddings = self.model.encode(sentences)\n",
                "\n",
                "        chunks = []\n",
                "        current_chunk = [sentences[0]]\n",
                "        current_embeddings = [embeddings[0]]\n",
                "\n",
                "        for i in range(1, len(sentences)):\n",
                "            similarity = cosine_similarity(\n",
                "                [embeddings[i]],\n",
                "                [np.mean(current_embeddings, axis=0)]\n",
                "            )[0][0]\n",
                "\n",
                "            # Start new chunk if topic shift detected\n",
                "            if (\n",
                "                similarity < self.similarity_threshold\n",
                "                or len(current_chunk) >= self.max_chunk_sentences\n",
                "            ):\n",
                "                chunks.append(\" \".join(current_chunk))\n",
                "                current_chunk = [sentences[i]]\n",
                "                current_embeddings = [embeddings[i]]\n",
                "            else:\n",
                "                current_chunk.append(sentences[i])\n",
                "                current_embeddings.append(embeddings[i])\n",
                "\n",
                "        # Final chunk\n",
                "        if current_chunk:\n",
                "            chunks.append(\" \".join(current_chunk))\n",
                "\n",
                "        return chunks\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting chunking process...\n",
                        "\n",
                        "✅ academic_and_professional_preparation_requirements_for_faculty.txt: Created 8 chunks.\n",
                        "✅ academic_credit_policy.txt: Created 52 chunks.\n",
                        "✅ academic_integrity_monitoring.txt: Created 7 chunks.\n",
                        "✅ academic_progress_policy_as_bus_jepson.txt: Created 12 chunks.\n",
                        "✅ acceptable_use_policy.txt: Created 21 chunks.\n",
                        "✅ access_to_electronic_files_policy.txt: Created 14 chunks.\n",
                        "✅ additional_compensation_for_staff.txt: Created 13 chunks.\n",
                        "✅ administrative_data_management_policy.txt: Created 28 chunks.\n",
                        "✅ alcohol_and_drug_policy.txt: Created 59 chunks.\n",
                        "✅ appointment_of_endowed_chairs.txt: Created 16 chunks.\n",
                        "✅ benefits_for_10-month_staff_and_faculty_on_9-month_contracts.txt: Created 4 chunks.\n",
                        "✅ bereavement_leave_policy.txt: Created 4 chunks.\n",
                        "✅ board_of_trustees_conflict_of_interest_policy.txt: Created 36 chunks.\n",
                        "✅ board_of_trustees_contract_approval_and_signature_authority.txt: Created 21 chunks.\n",
                        "✅ business_meals_policy.txt: Created 19 chunks.\n",
                        "✅ campus_ministry_policy.txt: Created 14 chunks.\n",
                        "✅ catering_minimums_policy.txt: Created 6 chunks.\n",
                        "✅ cell_phone-policy.txt: Created 7 chunks.\n",
                        "✅ classroom_scheduling_policy.txt: Created 5 chunks.\n",
                        "✅ compliance_training_policy.txt: Created 4 chunks.\n",
                        "✅ contract_management_policy.txt: Created 22 chunks.\n",
                        "✅ course_level_policy.txt: Created 6 chunks.\n",
                        "✅ cybersecurity_incident_response_policy.txt: Created 15 chunks.\n",
                        "✅ data_security_policy.txt: Created 42 chunks.\n",
                        "✅ delegation_of_approval_authority_and_processing_responsibilities.txt: Created 13 chunks.\n",
                        "✅ delegation_of_contract_approval_and_signature_authority_policy.txt: Created 37 chunks.\n",
                        "✅ designation_of_emergency_personnel_policy.txt: Created 10 chunks.\n",
                        "✅ early_retirement_plan_for_staff_and_faculty_with_a_continuing_appointment.txt: Created 25 chunks.\n",
                        "✅ effective_use_of_institutional_funds.txt: Created 11 chunks.\n",
                        "✅ electronic_card_access_policy.txt: Created 12 chunks.\n",
                        "✅ electronic_signature_policy.txt: Created 10 chunks.\n",
                        "✅ emergency_management_policy.txt: Created 17 chunks.\n",
                        "✅ endowment_spending_policy.txt: Created 6 chunks.\n",
                        "✅ external_data_transfer_policy.txt: Created 6 chunks.\n",
                        "✅ faculty_phased_retirement_plan.txt: Created 11 chunks.\n",
                        "✅ film_and_media_screening_policy.txt: Created 4 chunks.\n",
                        "✅ financial_aid_code_of_conduct.txt: Created 19 chunks.\n",
                        "✅ financial_conflict_of_interest_for_grant-funded_research.txt: Created 19 chunks.\n",
                        "✅ flexible_work_arrangement_policy.txt: Created 14 chunks.\n",
                        "✅ fringe_rate_policy.txt: Created 6 chunks.\n",
                        "✅ general_data_privacy_regulation_notice.txt: Created 33 chunks.\n",
                        "✅ gifts_and_gratuities_policy.txt: Created 4 chunks.\n",
                        "✅ gifts_prizes_and_awards_policy.txt: Created 10 chunks.\n",
                        "✅ hazing_policy.txt: Created 17 chunks.\n",
                        "✅ health_and_imunization_record_policy.txt: Created 5 chunks.\n",
                        "✅ hipaa_policy.txt: Created 35 chunks.\n",
                        "✅ holiday_leave_policy.txt: Created 10 chunks.\n",
                        "✅ identity_and_access_management_policy.txt: Created 23 chunks.\n",
                        "✅ inclement_weather_policy.txt: Created 8 chunks.\n",
                        "✅ inclement_weather_time_reporting_and_pay_policy.txt: Created 8 chunks.\n",
                        "✅ indirect_costs_recovery_policy.txt: Created 5 chunks.\n",
                        "✅ information_security_policy.txt: Created 12 chunks.\n",
                        "✅ intellectual_property_policy.txt: Created 53 chunks.\n",
                        "✅ international_travel_policy.txt: Created 14 chunks.\n",
                        "✅ joint_ventures_policy.txt: Created 5 chunks.\n",
                        "✅ jury_duty_and_subpoenas_policy.txt: Created 3 chunks.\n",
                        "✅ lock_and_key_management_policy.txt: Created 17 chunks.\n",
                        "✅ military_leave_policy.txt: Created 3 chunks.\n",
                        "✅ multiple_donor_gifts_policy.txt: Created 6 chunks.\n",
                        "✅ museum_collections_management_policy.txt: Created 44 chunks.\n",
                        "✅ nepotism_and_personal_relationship_policy.txt: Created 3 chunks.\n",
                        "✅ network_device_connectivity_policy.txt: Created 10 chunks.\n",
                        "✅ non-retaliation_policy.txt: Created 3 chunks.\n",
                        "✅ office_assignment_policy.txt: Created 11 chunks.\n",
                        "✅ official_university_communications_policy.txt: Created 2 chunks.\n",
                        "✅ outdoor_grill_use_policy.txt: Created 8 chunks.\n",
                        "✅ parental_leave_policy.txt: Created 10 chunks.\n",
                        "✅ password_policy.txt: Created 11 chunks.\n",
                        "✅ photography_and_videography_policy.txt: Created 7 chunks.\n",
                        "✅ placement_of_student_art_installations.txt: Created 7 chunks.\n",
                        "✅ policy_and_procedure_on_monetary_support_and_cash_donations.txt: Created 10 chunks.\n",
                        "✅ policy_for_employment_of_out_of_state_residents.txt: Created 14 chunks.\n",
                        "✅ policy_for_events_with_alcohol_on_campus.txt: Created 25 chunks.\n",
                        "✅ policy_for_responding_to_allegations_of_research_misconduct.txt: Created 99 chunks.\n",
                        "✅ policy_on_business_expenses_and_compensation.txt: Created 6 chunks.\n",
                        "✅ policy_on_campus_protests_and_demonstrations.txt: Created 18 chunks.\n",
                        "✅ policy_on_creating_suspending_eliminating_programs.txt: Created 32 chunks.\n",
                        "✅ policy_on_emeritus_status.txt: Created 3 chunks.\n",
                        "✅ policy_on_policies.txt: Created 9 chunks.\n",
                        "✅ policy_on_political_campaign_activity_on_campus.txt: Created 28 chunks.\n",
                        "✅ policy_on_pregnancy_childbirth_lactation_and_related_conditions_faculty_and_staff1.txt: Created 9 chunks.\n",
                        "✅ policy_on_prohibiting_and_responding_to_sexual_harassment_and_sexual_misconduct_faculty_staff.txt: Created 163 chunks.\n",
                        "✅ policy_on_prohibiting_and_responding_to_sexual_harassment_and_sexual_misconduct_students.txt: Created 130 chunks.\n",
                        "✅ policy_on_prohibiting_and_responding_to_sex_discrimination_faculty_staff.txt: Created 75 chunks.\n",
                        "✅ policy_on_prohibiting_and_responding_to_sex_discrimination_students.txt: Created 72 chunks.\n",
                        "✅ policy_on_protecting_student_privacy_in_distance_education.txt: Created 7 chunks.\n",
                        "✅ policy_on_provision_of_financial_resources_to_students.txt: Created 11 chunks.\n",
                        "✅ policy_on_space_allocation_and_facilities_resources.txt: Created 31 chunks.\n",
                        "✅ policy_prohibiting_and_responding_to_discrimination_based_on_protected_status_faculty_staff.txt: Created 69 chunks.\n",
                        "✅ policy_prohibiting_and_responding_to_discrimination_based_on_protected_status_students.txt: Created 67 chunks.\n",
                        "✅ policy_prohibiting_discrimination.txt: Created 19 chunks.\n",
                        "✅ policy_prohibiting_firearms_on_campus.txt: Created 5 chunks.\n",
                        "✅ space_taxonomy.txt: Created 3 chunks.\n",
                        "✅ Statement_on_Free_Expression.txt: Created 6 chunks.\n",
                        "✅ use_of_university_owned_houses.txt: Created 12 chunks.\n",
                        "\n",
                        "🎉 Total Chunks Created: 1965\n"
                    ]
                }
            ],
            "source": [
                "# Step 5: Load, Chunk, and Save Files\n",
                "from chromadb.utils import embedding_functions\n",
                "try:\n",
                "    # Try modern import first\n",
                "    from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
                "except ImportError:\n",
                "    # Fallback to legacy import\n",
                "    try:\n",
                "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "    except ImportError:\n",
                "        # Last resort\n",
                "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
                "\n",
                "embedding_function = embedding_functions.DefaultEmbeddingFunction()\n",
                "\n",
                "chunker = SemanticChunker(similarity_threshold=0.7)\n",
                "\n",
                "# Initialize Splitter (prioritize sentence boundaries)\n",
                "size_splitter = RecursiveCharacterTextSplitter(\n",
                "    separators=[\". \", \"? \", \"! \", \"\\n\", \" \", \"\"],\n",
                "    chunk_size=1000,      # Characters (~200 tokens)\n",
                "    chunk_overlap=100,    # Overlap to maintain context\n",
                "    length_function=len,\n",
                "    is_separator_regex=False\n",
                ")\n",
                "\n",
                "total_chunks_processed = 0\n",
                "\n",
                "print(\"Starting chunking process...\\n\")\n",
                "\n",
                "for file_name in source_files:\n",
                "    file_path = os.path.join(SOURCE_DIR, file_name)\n",
                "    \n",
                "    try:\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            text = f.read()\n",
                "            \n",
                "        # Create Chunks\n",
                "        #semantic_chunks = chunker.create_documents([text])\n",
                "        \n",
                "        semantic_chunks = chunker.chunk(text)\n",
                "        chunks = size_splitter.split_text(\"\\n\".join(semantic_chunks))\n",
                "        \n",
                "        # Save each chunk with metadata in filename\n",
                "        # Format: ch{index}-{original_name}-{metadata}.txt\n",
                "        base_name = os.path.splitext(file_name)[0]\n",
                "        \n",
                "        for i, chunk_content in enumerate(chunks):\n",
                "            # Metadata example: length of chunk\n",
                "            metadata_str = f\"len{len(chunk_content)}\"\n",
                "            chunk_filename = f\"ch{i+1}-{base_name}-{metadata_str}.txt\"\n",
                "            chunk_path = os.path.join(CHUNKED_DIR, chunk_filename)\n",
                "            \n",
                "            with open(chunk_path, 'w', encoding='utf-8') as chunk_file:\n",
                "                chunk_file.write(chunk_content)\n",
                "                \n",
                "        print(f\"✅ {file_name}: Created {len(chunks)} chunks.\")\n",
                "        total_chunks_processed += len(chunks)\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"❌ Error processing {file_name}: {e}\")\n",
                "\n",
                "print(f\"\\n🎉 Total Chunks Created: {total_chunks_processed}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Content of ch1-academic_and_professional_preparation_requirements_for_faculty-len495.txt ---\n",
                        "university of richmond | 1\n",
                        "university of richmond\n",
                        "policy manual\n",
                        "purpose:\n",
                        "this policy is designed to ensure that faculty at the university of richmond have the highest quality\n",
                        "preparation to accomplish the educational mission of the university and to ensure that the university of\n",
                        "richmond's academic and professional preparation requirements for full-time and part-time faculty conform\n",
                        "to the criteria established by the commission on colleges of the southern association of colleges and\n",
                        "schools\n",
                        "\n",
                        "--- End of Sample ---\n"
                    ]
                }
            ],
            "source": [
                "# Step 6: Verify a Sample Chunk\n",
                "# Check one of the generated files to ensure content is correct\n",
                "if os.listdir(CHUNKED_DIR):\n",
                "    sample_chunk = os.listdir(CHUNKED_DIR)[0]\n",
                "    sample_path = os.path.join(CHUNKED_DIR, sample_chunk)\n",
                "    \n",
                "    print(f\"--- Content of {sample_chunk} ---\")\n",
                "    with open(sample_path, 'r', encoding='utf-8') as f:\n",
                "        print(f.read()[:500]) # Print first 500 chars\n",
                "    print(\"\\n--- End of Sample ---\")\n",
                "else:\n",
                "    print(\"No chunks found to verify.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 3: Embeddings & Vector Store\n",
                "We will now read the chunked files we just created, generate embeddings (handled automatically by Chroma's default embedding function), and upsert them into the ChromaDB Cloud collection.\n",
                "\n",
                "> **Note:** We are using ChromaDB's default embedding model (`all-MiniLM-L6-v2`) which is built into the client. No extra API calls to Bedrock are needed for *embedding* in this setup, saving costs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 3810 chunk files to process.\n",
                        "Prepared 3810 documents for embedding.\n"
                    ]
                }
            ],
            "source": [
                "# Step 7: Prepare Data for Embedding\n",
                "import uuid\n",
                "import re\n",
                "\n",
                "chunked_files = [f for f in os.listdir(CHUNKED_DIR) if f.endswith('.txt')]\n",
                "\n",
                "documents = []\n",
                "metadatas = []\n",
                "ids = []\n",
                "\n",
                "print(f\"Found {len(chunked_files)} chunk files to process.\")\n",
                "\n",
                "for file_name in chunked_files:\n",
                "    file_path = os.path.join(CHUNKED_DIR, file_name)\n",
                "    \n",
                "    try:\n",
                "        with open(file_path, 'r', encoding='utf-8') as f:\n",
                "            content = f.read()\n",
                "            \n",
                "        # Parse Metadata from Filename\n",
                "        # Format: ch{index}-{original_name}-{len}.txt\n",
                "        # Example: ch1-academic_policy-len495.txt\n",
                "        try:\n",
                "            name_no_ext = os.path.splitext(file_name)[0]\n",
                "            parts = name_no_ext.split('-')\n",
                "            \n",
                "            # 1. Chunk Part (first item, e.g., 'ch1')\n",
                "            chunk_part = int(parts[0].replace('ch', ''))\n",
                "            \n",
                "            # 2. Size (last item, e.g., 'len495')\n",
                "            size = int(parts[-1].replace('len', ''))\n",
                "            \n",
                "            # 3. File Name (everything in between)\n",
                "            original_filename = \"-\".join(parts[1:-1])\n",
                "            \n",
                "            meta = {\n",
                "                \"source\": file_name,\n",
                "                \"file_name\": original_filename,\n",
                "                \"chunk_part\": chunk_part,\n",
                "                \"size\": size\n",
                "            }\n",
                "        except Exception as e:\n",
                "            # Fallback if naming convention doesn't match\n",
                "            print(f\"⚠️ Metadata parse warning for {file_name}: {e}\")\n",
                "            meta = {\"source\": file_name}\n",
                "\n",
                "        # Add to lists\n",
                "        documents.append(content)\n",
                "        metadatas.append(meta)\n",
                "        ids.append(str(uuid.uuid4()))\n",
                "        \n",
                "    except Exception as e:\n",
                "        print(f\"Warning: Could not read {file_name}: {e}\")\n",
                "\n",
                "print(f\"Prepared {len(documents)} documents for embedding.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Upserting documents to ChromaDB Collection in batches...\n",
                        "   ✅ Processed batch 0 to 100\n",
                        "   ✅ Processed batch 100 to 200\n",
                        "   ✅ Processed batch 200 to 300\n",
                        "   ✅ Processed batch 300 to 400\n",
                        "   ✅ Processed batch 400 to 500\n",
                        "   ✅ Processed batch 500 to 600\n",
                        "   ✅ Processed batch 600 to 700\n",
                        "   ✅ Processed batch 700 to 800\n",
                        "   ✅ Processed batch 800 to 900\n",
                        "   ✅ Processed batch 900 to 1000\n",
                        "   ✅ Processed batch 1000 to 1100\n",
                        "   ✅ Processed batch 1100 to 1200\n",
                        "   ✅ Processed batch 1200 to 1300\n",
                        "   ✅ Processed batch 1300 to 1400\n",
                        "   ✅ Processed batch 1400 to 1500\n",
                        "   ✅ Processed batch 1500 to 1600\n",
                        "   ✅ Processed batch 1600 to 1700\n",
                        "   ✅ Processed batch 1700 to 1800\n",
                        "   ✅ Processed batch 1800 to 1900\n",
                        "   ✅ Processed batch 1900 to 2000\n",
                        "   ✅ Processed batch 2000 to 2100\n",
                        "   ✅ Processed batch 2100 to 2200\n",
                        "   ✅ Processed batch 2200 to 2300\n",
                        "   ✅ Processed batch 2300 to 2400\n",
                        "   ✅ Processed batch 2400 to 2500\n",
                        "   ✅ Processed batch 2500 to 2600\n",
                        "   ✅ Processed batch 2600 to 2700\n",
                        "   ✅ Processed batch 2700 to 2800\n",
                        "   ✅ Processed batch 2800 to 2900\n",
                        "   ✅ Processed batch 2900 to 3000\n",
                        "   ✅ Processed batch 3000 to 3100\n",
                        "   ✅ Processed batch 3100 to 3200\n",
                        "   ✅ Processed batch 3200 to 3300\n",
                        "   ✅ Processed batch 3300 to 3400\n",
                        "   ✅ Processed batch 3400 to 3500\n",
                        "   ✅ Processed batch 3500 to 3600\n",
                        "   ✅ Processed batch 3600 to 3700\n",
                        "   ✅ Processed batch 3700 to 3800\n",
                        "   ✅ Processed batch 3800 to 3810\n",
                        "\n",
                        "🎉 Successfully added all 3810 documents to ChromaDB!\n",
                        "Final Collection Count: 3810\n"
                    ]
                }
            ],
            "source": [
                "# Step 8: Add to ChromaDB (Embed & Upsert)\n",
                "# Batch size limit for Chroma is usually 1000 (we hit 1914!), so we must batch.\n",
                "print(\"Upserting documents to ChromaDB Collection in batches...\")\n",
                "\n",
                "BATCH_SIZE = 100  # Safe batch size\n",
                "total_docs = len(documents)\n",
                "\n",
                "try:\n",
                "    for i in range(0, total_docs, BATCH_SIZE):\n",
                "        batch_docs = documents[i : i + BATCH_SIZE]\n",
                "        batch_metas = metadatas[i : i + BATCH_SIZE]\n",
                "        batch_ids = ids[i : i + BATCH_SIZE]\n",
                "        \n",
                "        collection.add(\n",
                "            documents=batch_docs,\n",
                "            metadatas=batch_metas,\n",
                "            ids=batch_ids\n",
                "        )\n",
                "        print(f\"   ✅ Processed batch {i} to {min(i+BATCH_SIZE, total_docs)}\")\n",
                "        \n",
                "    print(f\"\\n🎉 Successfully added all {total_docs} documents to ChromaDB!\")\n",
                "    print(f\"Final Collection Count: {collection.count()}\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error adding to ChromaDB: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Querying ChromaDB for: 'What is the main topic of these documents?'...\n",
                        "\n",
                        "[Result 1]\n",
                        "   File: hipaa_policy\n",
                        "   Part: 23\n",
                        "   Size: 931\n",
                        "   Snippet: . the library is located in\n",
                        "box for internal access. outdated and superseded materials from the secu...\n",
                        "\n",
                        "[Result 2]\n",
                        "   File: hipaa_policy\n",
                        "   Part: 21\n",
                        "   Size: 997\n",
                        "   Snippet: . the university will maintain documentation, in written or electronic form, of policies, procedures...\n",
                        "\n",
                        "[Result 3]\n",
                        "   File: policy_on_prohibiting_and_responding_to_sex_discrimination_faculty_staff\n",
                        "   Part: 56\n",
                        "   Size: 168\n",
                        "   Snippet: . review investigative report.\n",
                        "the title ix coordinator shall review the final investigative report\n",
                        "...\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Step 9: Verify Embedding with a Test Query\n",
                "# We will perform a simple similarity search (no LLM yet) to see if we get relevant chunks.\n",
                "\n",
                "query_text = \"What is the main topic of these documents?\"  # Replace with a relevant question for your data\n",
                "\n",
                "print(f\"Querying ChromaDB for: '{query_text}'...\\n\")\n",
                "\n",
                "results = collection.query(\n",
                "    query_texts=[query_text],\n",
                "    n_results=3 # Get top 3 matches\n",
                ")\n",
                "\n",
                "if results['documents']:\n",
                "    for i, doc in enumerate(results['documents'][0]):\n",
                "        meta = results['metadatas'][0][i]\n",
                "        print(f\"[Result {i+1}]\")\n",
                "        print(f\"   File: {meta.get('file_name', 'Unknown')}\")\n",
                "        print(f\"   Part: {meta.get('chunk_part', '?')}\")\n",
                "        print(f\"   Size: {meta.get('size', '?')}\")\n",
                "        print(f\"   Snippet: {doc[:100]}...\\n\")\n",
                "else:\n",
                "    print(\"No results found. Check if documents were added correctly.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Phase 4: Retrieval & Generation\n",
                "We implement the custom retrieval logical (with distince threshold filtering) and connect it to AWS Bedrock for the final answer generation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 10: Custom Retrieval Function\n",
                "def retrieve_documents(query, n_results=5, threshold=1.5, filter_by=None):\n",
                "    \"\"\"\n",
                "    Retrieve relevant documents with distance threshold filtering.\n",
                "    \n",
                "    Args:\n",
                "        query: The search query string\n",
                "        n_results: Max results to return initially\n",
                "        threshold: Max distance (lower = more strict match). \n",
                "                   For Cosine distance: 0 is identical, 1 is orthogonal, 2 is opposite.\n",
                "                   Typical good matches are < 1.0 depending on embedding model.\n",
                "        filter_by: Metadata filter dict (optional)\n",
                "    \n",
                "    Returns:\n",
                "        List of dicts: {text, source, distance}\n",
                "    \"\"\"\n",
                "    #print(f\"Filter by: {filter_by}\")\n",
                "    #Add LLM here to ask what documents have the info\n",
                "    metadata = collection.get(include=[\"metadatas\"])\n",
                "    file_names = sorted({\n",
                "        meta[\"file_name\"]\n",
                "        for meta in metadata[\"metadatas\"]\n",
                "        if \"file_name\" in meta\n",
                "    })\n",
                "    # Query Chroma\n",
                "    results = collection.query(\n",
                "        query_texts=[query],\n",
                "        n_results=n_results,\n",
                "        where=filter_by,\n",
                "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
                "    )\n",
                "    #print(results)\n",
                "    \n",
                "    docs = []\n",
                "    \n",
                "    # Check if we got results\n",
                "    if results['documents'] and results['documents'][0]:\n",
                "        # Iterate through the first query's results\n",
                "        for text, meta, dist in zip(\n",
                "            results['documents'][0],\n",
                "            results['metadatas'][0],\n",
                "            results['distances'][0]\n",
                "        ):\n",
                "            # Filter by threshold\n",
                "            if dist <= threshold:\n",
                "                docs.append({\n",
                "                    \"text\": text,\n",
                "                    \"source\": meta.get(\"source\", \"unknown\"),\n",
                "                    \"distance\": dist\n",
                "                })\n",
                "                \n",
                "    print(f\"✅ Retrieved {len(docs)} documents (Threshold: {threshold})\")\n",
                "    return docs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 11: RAG Generation Function (Bedrock)\n",
                "from langchain_aws import ChatBedrock\n",
                "\n",
                "# Fix for newer LangChain versions (v0.1+)\n",
                "try:\n",
                "    from langchain_core.prompts import PromptTemplate\n",
                "    from langchain_core.runnables import RunnablePassthrough\n",
                "    from langchain_core.output_parsers import StrOutputParser\n",
                "except ImportError:\n",
                "    # Fallback for older versions\n",
                "    from langchain.prompts import PromptTemplate\n",
                "    from langchain.schema.runnable import RunnablePassthrough\n",
                "    from langchain.schema.output_parser import StrOutputParser\n",
                "\n",
                "# Initialize LLM\n",
                "llm = ChatBedrock(\n",
                "    model_id=BEDROCK_MODEL_ID,\n",
                "    client=bedrock_client,\n",
                "    model_kwargs={\"max_tokens\": 1000, \"temperature\": 0.1} # Claude 3 uses 'max_tokens'\n",
                ")\n",
                "\n",
                "def generate_answer(query):\n",
                "    # 1. Retrieve Context\n",
                "    results = collection.get(include=[\"metadatas\"])\n",
                "\n",
                "    unique_filenames = set()\n",
                "\n",
                "    metadata = results[\"metadatas\"]\n",
                "    for data in metadata:\n",
                "        unique_filenames.add(data['file_name'])\n",
                "    \n",
                "    #print(unique_filenames)\n",
                "\n",
                "    source_selection_prompt = f\"\"\"\n",
                "You are given a list of document sources.\n",
                "Return ONLY the names of the sources that are relevant to the question.\n",
                "If none are relevant, return an empty list.\n",
                "Do not explain your reasoning.\n",
                "\n",
                "Sources:\n",
                "{', '.join(unique_filenames)}\n",
                "\n",
                "Question:\n",
                "{query}\n",
                "\n",
                "Return format:\n",
                "source1, source2, source3\n",
                "    \"\"\"\n",
                "    \n",
                "    source_response = llm.invoke(source_selection_prompt).content.strip()\n",
                "\n",
                "    if source_response:\n",
                "        selected_sources = [s.strip() for s in source_response.split(\",\")]\n",
                "    else:\n",
                "        selected_sources = []\n",
                "\n",
                "    #print(selected_sources)\n",
                "\n",
                "    relevant_docs = retrieve_documents(query, n_results=5, threshold=1.2, filter_by={\"file_name\": {\"$in\": selected_sources}})\n",
                "    #relevant_docs = retrieve_documents(query, n_results=5, threshold=1.2)\n",
                "    \n",
                "    if not relevant_docs:\n",
                "        return \"I could not find any relevant information to answer your question.\"\n",
                "    \n",
                "    # 2. Format Context\n",
                "    context_text = \"\\n\\n\".join([f\"[Source: {d['source']}]\\n{d['text']}\" for d in relevant_docs])\n",
                "    \n",
                "    # 3. Construct Prompt\n",
                "    prompt_template = \"\"\"\n",
                "    Human: You are a concise and direct assistant. Use the following pieces of context to answer the question at the end.\n",
                "    \n",
                "    Rules for answering:\n",
                "    1. Be extremely concise.\n",
                "    2. Do NOT use bullet points or numbered lists. \n",
                "    3. Provide a single, direct paragraph.\n",
                "    4. If you don't know the answer, just say that you don't know.\n",
                "\n",
                "    Context:\n",
                "    {context}\n",
                "\n",
                "    Question: {question}\n",
                "\n",
                "    Assistant:\"\"\"\n",
                "    \n",
                "    prompt = PromptTemplate(\n",
                "        template=prompt_template, \n",
                "        input_variables=[\"context\", \"question\"]\n",
                "    )\n",
                "    \n",
                "    # 4. Invoke Chain manually (since we have custom retrieval logic)\n",
                "    # We format the prompt first, then send to LLM\n",
                "    final_prompt = prompt.format(context=context_text, question=query)\n",
                "    response = llm.invoke(final_prompt)\n",
                "    \n",
                "    return response.content"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "❓ Question: What is the policy regarding drug usage?\n",
                        "\n",
                        "✅ Retrieved 5 documents (Threshold: 1.2)\n",
                        "💡 Answer:\n",
                        "The university has a strict policy prohibiting the unlawful manufacture, possession, use, or distribution of illegal drugs, marijuana, and/or alcohol by employees and students on university property or as part of university activities. Violations can result in disciplinary actions ranging from substance education to permanent separation or dismissal from employment. The university reserves the right to conduct drug/alcohol testing for reasonable suspicion and may refer violators for criminal prosecution.\n"
                    ]
                }
            ],
            "source": [
                "# Step 12: Final Test\n",
                "query = \"What is the policy regarding drug usage?\"\n",
                "\n",
                "print(f\"❓ Question: {query}\\n\")\n",
                "\n",
                "answer = generate_answer(query)\n",
                "\n",
                "print(\"💡 Answer:\")\n",
                "print(answer)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
