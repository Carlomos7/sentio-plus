{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Sentio+ RAG-Optimized Preprocessing\n",
                "\n",
                "This notebook implements the **Hybrid Stratified Signal** sampling strategy to create a high-signal corpus for the Sentio+ RAG chatbot.\n",
                "\n",
                "**Core Logic:**\n",
                "- **Source:** `apps_reviews.csv` and `apps_info.csv` (Kaggle cache)\n",
                "- **Filter:** Minimum 150 characters for high-density insights.\n",
                "- **Sample:** 50,000 reviews balanced across categories and ratings.\n",
                "- **Technique:** Stratified sampling prioritizing **Length** and **Helpfulness**.\n",
                "- **Output:** `sentio_plus_rag_ready.csv` with Context Headers."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Imports and Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Environment ready.\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from pathlib import Path\n",
                "from datetime import datetime, timedelta\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "pd.set_option('display.max_columns', None)\n",
                "pd.set_option('display.max_colwidth', 200)\n",
                "\n",
                "print(\"Environment ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Raw Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading data from: /Users/chenchenliu/.cache/kagglehub/datasets/dmytrobuhai/play-market-2025-1m-reviews-500-titles/versions/1\n",
                        "Raw Reviews: 466,700\n",
                        "Raw App Info: 217\n"
                    ]
                }
            ],
            "source": [
                "# Path to Kaggle cached data\n",
                "DATA_PATH = Path.home() / \".cache/kagglehub/datasets/dmytrobuhai/play-market-2025-1m-reviews-500-titles/versions/1\"\n",
                "\n",
                "print(f\"Loading data from: {DATA_PATH}\")\n",
                "\n",
                "reviews_df = pd.read_csv(DATA_PATH / \"apps_reviews.csv\")\n",
                "info_df = pd.read_csv(DATA_PATH / \"apps_info.csv\")\n",
                "\n",
                "print(f\"Raw Reviews: {len(reviews_df):,}\")\n",
                "print(f\"Raw App Info: {len(info_df):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Integration & Cleaning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Integrated dataset: 466,700\n"
                    ]
                }
            ],
            "source": [
                "def clean_category(cat_str):\n",
                "    if pd.isna(cat_str): return \"Unknown\"\n",
                "    if \",\" in cat_str: return cat_str.split(\",\")[-1].strip()\n",
                "    return cat_str.strip()\n",
                "\n",
                "# 1. Clean categories\n",
                "info_df['category'] = info_df['categories'].apply(clean_category)\n",
                "\n",
                "# 2. Merge\n",
                "merged_df = reviews_df.merge(\n",
                "    info_df[['app_id', 'app_name', 'category', 'content_rating', 'score', 'downloads']],\n",
                "    on='app_id', \n",
                "    how='left'\n",
                ")\n",
                "\n",
                "# 3. Initial Clean\n",
                "merged_df = merged_df.dropna(subset=['review_text', 'app_name', 'category'])\n",
                "\n",
                "print(f\"Integrated dataset: {len(merged_df):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. The Quality Gate (Filtering)\n",
                "\n",
                "- **Length Threshold:** > 150 characters\n",
                "- **Date Handling:** Identify recent reviews (Last 12 months)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "High-Quality Pool (>150 chars): 222,320\n",
                        "Recent reviews (Last 12m): 52,558\n"
                    ]
                }
            ],
            "source": [
                "# Calculate text length\n",
                "merged_df['text_length'] = merged_df['review_text'].str.len()\n",
                "\n",
                "# Filter for length (The Quality Gate)\n",
                "high_quality_pool = merged_df[merged_df['text_length'] > 150].copy()\n",
                "\n",
                "# Handle dates\n",
                "high_quality_pool['review_date'] = pd.to_datetime(high_quality_pool['review_date'])\n",
                "CUTOFF_DATE = high_quality_pool['review_date'].max() - timedelta(days=365)\n",
                "high_quality_pool['is_recent'] = high_quality_pool['review_date'] >= CUTOFF_DATE\n",
                "\n",
                "print(f\"High-Quality Pool (>150 chars): {len(high_quality_pool):,}\")\n",
                "print(f\"Recent reviews (Last 12m): {high_quality_pool['is_recent'].sum():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Hybrid Stratified Signal Sampling\n",
                "\n",
                "We sample 50,000 reviews ensuring breadth (Category/Rating) and depth (Length/Helpful)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Buckets: 90\n",
                        "Target per bucket: 555\n",
                        "\n",
                        "Final Sample Size: 50,000\n"
                    ]
                }
            ],
            "source": [
                "TARGET_TOTAL = 50000\n",
                "categories = high_quality_pool['category'].unique()\n",
                "ratings = sorted(high_quality_pool['review_score'].unique())\n",
                "\n",
                "# Calculate target per (cat, rating) bucket\n",
                "n_buckets = len(categories) * len(ratings)\n",
                "target_per_bucket = TARGET_TOTAL // n_buckets\n",
                "\n",
                "print(f\"Buckets: {n_buckets}\")\n",
                "print(f\"Target per bucket: {target_per_bucket}\")\n",
                "\n",
                "sampled_dfs = []\n",
                "\n",
                "for cat in categories:\n",
                "    for rating in ratings:\n",
                "        bucket = high_quality_pool[\n",
                "            (high_quality_pool['category'] == cat) & \n",
                "            (high_quality_pool['review_score'] == rating)\n",
                "        ]\n",
                "        \n",
                "        if len(bucket) == 0: continue\n",
                "        \n",
                "        # Signal Sorting: Prioritize Length and Helpfulness\n",
                "        # Within bucket, we want 60% recent if possible\n",
                "        n_recent_target = int(target_per_bucket * 0.6)\n",
                "        n_older_target = target_per_bucket - n_recent_target\n",
                "        \n",
                "        recent_bucket = bucket[bucket['is_recent']].sort_values(['text_length', 'helpful_count'], ascending=False)\n",
                "        older_bucket = bucket[~bucket['is_recent']].sort_values(['text_length', 'helpful_count'], ascending=False)\n",
                "        \n",
                "        # Extract segments\n",
                "        recent_sample = recent_bucket.head(n_recent_target)\n",
                "        older_sample = older_bucket.head(target_per_bucket - len(recent_sample))\n",
                "        \n",
                "        bucket_sample = pd.concat([recent_sample, older_sample])\n",
                "        \n",
                "        # If still short, take whatever is left from the bucket\n",
                "        if len(bucket_sample) < target_per_bucket:\n",
                "            remaining = bucket[~bucket.index.isin(bucket_sample.index)].sort_values(['text_length', 'helpful_count'], ascending=False)\n",
                "            bucket_sample = pd.concat([bucket_sample, remaining.head(target_per_bucket - len(bucket_sample))])\n",
                "            \n",
                "        sampled_dfs.append(bucket_sample)\n",
                "\n",
                "final_sampled_df = pd.concat(sampled_dfs, ignore_index=True)\n",
                "\n",
                "# If we are still short of 50k (due to small buckets), fill from the remaining high-quality pool\n",
                "if len(final_sampled_df) < TARGET_TOTAL:\n",
                "    needed = TARGET_TOTAL - len(final_sampled_df)\n",
                "    remaining_pool = high_quality_pool[~high_quality_pool.index.isin(final_sampled_df.index)]\n",
                "    top_up = remaining_pool.sort_values(['is_recent', 'text_length', 'helpful_count'], ascending=False).head(needed)\n",
                "    final_sampled_df = pd.concat([final_sampled_df, top_up], ignore_index=True)\n",
                "\n",
                "print(f\"\\nFinal Sample Size: {len(final_sampled_df):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Metadata Enrichment (The Context Header)\n",
                "\n",
                "Format: `[APP: {app_name} | CAT: {category} | RATING: {rating}/5 | DATE: {date} | SEGMENT: {content_rating}] USER REVIEW: {review_text}`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Enrichment complete. Example:\n",
                        "[APP: Google Wallet | CAT: Finance | RATING: 1/5 | DATE: 2024-09 | SEGMENT: Everyone] USER REVIEW: I used to love this app, I've got all my loyalty cards and that stored in it, but lately, a security update requires verification for every purchase. And it doesn't seem to even make any difference if the phone is unlocked or not, I have to go into the app, click 'Verify it's you', and then put in my pin again. It often fails the transaction first time, so then I'll have to unlock my phone, try again, fail again, then put in my code. The whole experience is a thousand times clunkier than before.\n"
                    ]
                }
            ],
            "source": [
                "def enrich_review(row):\n",
                "    header = (\n",
                "        f\"[APP: {row['app_name']} | \"\n",
                "        f\"CAT: {row['category']} | \"\n",
                "        f\"RATING: {int(row['review_score'])}/5 | \"\n",
                "        f\"DATE: {row['review_date'].strftime('%Y-%m')} | \"\n",
                "        f\"SEGMENT: {row['content_rating']}] \"\n",
                "        f\"USER REVIEW: {row['review_text']}\"\n",
                "    )\n",
                "    return header\n",
                "\n",
                "final_sampled_df['enriched_text'] = final_sampled_df.apply(enrich_review, axis=1)\n",
                "\n",
                "print(\"Enrichment complete. Example:\")\n",
                "print(final_sampled_df['enriched_text'].iloc[0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Final Preparation & Export"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "✅ Saved RAG-Ready dataset to: data/processed/sentio_plus_rag_ready.csv\n",
                        "Total Evidence Chunks: 50,000\n"
                    ]
                }
            ],
            "source": [
                "# Map column names to final spec\n",
                "final_df = final_sampled_df[[\n",
                "    'app_id', 'app_name', 'category', 'review_score', 'review_date', \n",
                "    'helpful_count', 'content_rating', 'score', 'downloads', 'enriched_text', 'text_length'\n",
                "]].copy()\n",
                "\n",
                "final_df = final_df.rename(columns={\n",
                "    'review_score': 'rating',\n",
                "    'score': 'app_avg_score'\n",
                "})\n",
                "\n",
                "# Add ID\n",
                "final_df['review_id'] = range(1, len(final_df) + 1)\n",
                "\n",
                "# Reorder\n",
                "final_df = final_df[[\n",
                "    'review_id', 'app_id', 'app_name', 'category', 'rating', 'review_date', \n",
                "    'helpful_count', 'content_rating', 'app_avg_score', 'downloads', 'text_length', 'enriched_text'\n",
                "]]\n",
                "\n",
                "# Save\n",
                "OUTPUT_DIR = Path(\"data/processed\")\n",
                "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
                "OUTPUT_FILE = OUTPUT_DIR / \"sentio_plus_rag_ready.csv\"\n",
                "\n",
                "final_df.to_csv(OUTPUT_FILE, index=False)\n",
                "\n",
                "print(f\"\\n✅ Saved RAG-Ready dataset to: {OUTPUT_FILE}\")\n",
                "print(f\"Total Evidence Chunks: {len(final_df):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Data Distribution Audit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== Rating Distribution ===\n",
                        "rating\n",
                        "1    13997\n",
                        "2     9665\n",
                        "3     8763\n",
                        "4     8443\n",
                        "5     9132\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "=== Top 10 Categories ===\n",
                        "category\n",
                        "Food & Drink     4655\n",
                        "Entertainment    4611\n",
                        "Business         4141\n",
                        "Social           3855\n",
                        "Shopping         3668\n",
                        "Productivity     3649\n",
                        "Communication    3609\n",
                        "Finance          3470\n",
                        "House & Home     2817\n",
                        "Education        2605\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "=== Signal Metrics ===\n",
                        "Average Review Length: 404.9 chars\n",
                        "Total Helpful Votes: 2,560,996\n"
                    ]
                }
            ],
            "source": [
                "print(\"=== Rating Distribution ===\")\n",
                "print(final_df['rating'].value_counts().sort_index())\n",
                "\n",
                "print(\"\\n=== Top 10 Categories ===\")\n",
                "print(final_df['category'].value_counts().head(10))\n",
                "\n",
                "print(f\"\\n=== Signal Metrics ===\")\n",
                "print(f\"Average Review Length: {final_df['text_length'].mean():.1f} chars\")\n",
                "print(f\"Total Helpful Votes: {final_df['helpful_count'].sum():,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Successfully saved 50,000 reviews to data/processed/THIS_ONE.csv\n"
                    ]
                }
            ],
            "source": [
                "# Define the output location\n",
                "import os\n",
                "output_path = \"data/processed/sentio_plus_rag_ready.csv\"\n",
                "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
                "\n",
                "# Export the DataFrame\n",
                "# index=False ensures you don't save the row numbers as a separate column\n",
                "final_df.to_csv(output_path, index=False)\n",
                "\n",
                "print(f\"Successfully saved {len(final_df):,} reviews to {output_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
